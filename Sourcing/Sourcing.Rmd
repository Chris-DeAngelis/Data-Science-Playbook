---
title: "Sourcing Data"
author: "Chris DeAngelis, CFA"
date: "12/31/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load required packages
library(tidyverse)
```

## Credential Encryption

It is strongly advised to encrypt sensitive information related to connecting to data. Never hard code these credentials in your code. In R, I recommend using Keyring. In place of stored variables, RStudio can halt during running a script and prompt users to provide credentials. The prompt is saved to a variable and the script finishes executing. Keyring is an R package that requires a master password to access all stored credentials. I maintain all my API access credentials in Keyring.

```{r API}
# Save credentials securely
library(keyring)

# Set up keyring
keyring_create()

# View available keys. You must know the name of the keyring you've created
key_list(keyring="Elkay")

# Assign keys
key_set_with_value(service = "Database",
                   username = "DS",
                   password = "PW",
                   keyring = "Elkay")

# Access keys
key_get(service = "Snowflake",
        username = "DATASCIENCE_READONLY",
        keyring = "Elkay")
```

## Standard Filetypes

R and Python are very accommodating to several file types. Some of the most common file types (and their advantages) are listed below:
1. CSV - small file size, organized, minimal formatting, works on several platforms
2. XLSX - most common file type, can include multiple tabs of data
3. RDS - very small file size, specific to RStudio but allows for easy saving/loading of data objects


```{r fileTypes}
library(tidyverse) # Connecting to standard file formats
library(readxl) # Connecting to Excel file types (.XLS,.XLSX)

# CSV - file name is only required parameter. Skip is useful for skipping rows. Avoid factor coercing using stringsAsFactors=FALSE
read_csv("filepath/filename.csv",skip=1,stringsAsFactors=FALSE)
write_csv(dataframe,"filepath/filename.csv")

# Excel - If not specified, only the FIRST, single worksheet will be read in
read_excel("filepath/filename.xlsx",sheet="sheetname")
# Writing to Excel can be done but involves a considerable amount of work and is outside the scope of this documentation. Consider using Power Automate instead

# Multiple Excel worksheets
tab_names <- c("list","of","tab","names")
all_sheets <- lapply(tab_names, function(x) read_excel(path="filepath/filename.xlsx",sheet=x))

# RDS
readRDS("filepath/filename.RDS")
saveRDS(dataframe,"filepath/filename.RDS")
```

## Database Connections

There are several different types of databases and ways of accessing databases. I will focus on Snowflake as it is Elkay's intentions to support Data Science initiatives through Snowflake's Data Warehouse and Datalake. R has multiple packages that support database connections but I prefer working with the DBI package. 

General instructions for reading & writing to a database:

1. Obtain database credentials: server, database, warehouse (sometimes optional), userID (UID), and password. Do NOT hardcode these credentials in the code
2. Download any necessary drivers. Snowflake requires this and occasionally these will need to be updated
https://www.snowflake.com/drivers ???
3. Establish connection with the database and store the connection in an object
4. Write data query/queries
5. Send query. If reading from a database - store results. If writing, pass along the data in a dataframe (or matrix?) object
6. Once finished accessing the database, it is best practice to close the connection

```{r database}
# Save credentials securely
library(keyring)
library(DBI)

# Establish connection
write_connection <- DBI::dbConnect(odbc::odbc(),
                                   driver = "SnowflakeDSIIDriver",
                                   server = "elkay.east-us-2.azure.snowflakecomputing.com",
                                   database = "DATALAKE",
                                   Warehouse = "DATASCIENCE_WH",
                                   UID    = "DATALAKE_READ_WRITE_CREATE", #rstudioapi::askForPassword("Datalake_readonly"),
                                   PWD    = key_get("Snowflake_Write","DATALAKE_READ_WRITE_CREATE", "Elkay"))#rstudioapi::askForPassword("Database password"))

table_id <- Id(database="DATALAKE", schema="SERVING", table=table)
dbWriteTable(write_connection, table_id, data, overwrite = TRUE, row.names = FALSE)

# Close connection
dbDisconnect(write_connection)

# Supporting Code
source("./Capabilities/Souring/Database/snowflake.R")
```

## Google Sheets

A few Elkay teams (and third party agencies) use Google. R can read in Google data fairly easily. Oftentimes issues are a result of proper access. Please consult data owner

```{r Gsheet}
# Load packages
library(gsheet)

# Source data
print_media <- gsheet2tbl('https://docs.google.com/spreadsheets/d/1z2xDQBtECJzPgAaT4FzM2jA90BqYcBg-W1us7UYBir8/edit#gid=0')

```

## Accessing APIs

API access and instructions vary by platform. A free, useful tool for setting up access is Postman:
https://www.Postman

As of 12/31/2021, Elkay's Data Science team has successfully connected to the following APIs:
1. Google Analytics
2. Google Ads
3. Amazon
4. Facebook???
5. Home Depot???
6. Wayfair???
7. USPTO (US Patent Office)

APIs can include write access to sensitive data so it is critical to encrypt and avoid sharing API access credentials. It is also smart to not hard code any credentials in your code. Although setup varies by API, here is a standard approach to setting up access.

1. Consult platform API instructions, guidelines, and requirements
2. Obtain a client ID and client secret. This can be provided PER USER and should NOT be shared
3. Obtain an access token. Oftentimes this can be done in your code using a provided URL and submitting a proper header with your client ID and client secret credentials. Remember access tokens will expire and oftentimes are only valid for your working session
4. Submit a GET request using the access token to establish a connection. You can store the result or view the status message to confirm the request was received successfully
5. Once connected, APIs each have their own set of functions and capabilities. Use their API documentation to access the data you are looking for. Be aware of thresholds and limitations
6. If you've established an open connection, it is best practice to close your connection at the end of your session

The API example below is specific to connecting to Amazon's Marketing API. I recommend using Keyring in R (documented separately) to encrypt credentials.
```{r API}
# Save credentials securely
library(keyring)

# Authenticate App
fb_token <- "EAAFRgPPZBldIBAMUuYIEPWDW9XRtidL1KQkKXGzkJXnKO5N2Fns5cRRCpvZBUJCDiVqC6saaTfAyrEgWz7bnDCqXfPBCHEZA475H0GBOHcWcsb5XAQFZCF7UHVtdlhDVIKnBSFKnLwq05xAhknrXozzayvOcULDyefUz89lHcQZDZD"
fb_account_id <- "518916408184112"
fb_app_id <- "371089267922386"
fb_app_secret <- "01961961b09a5590bcde4dd73f059dd7"
fb_api_version <- "v11.0"
fb_business_id <- "129401008162242"
fb_username <- "deangecd@muohio.edu"

fb_auth <- fbAuth(username = fb_username,
                  app_id = fb_app_id,
                  app_secret = fb_app_secret,
                  token_path = fb_token,
                  reauth = FALSE,
                  skip_option = FALSE)
fb_token <- substr(fb_auth,2,nchar(fb_auth)-1)
```

## Web Scraping

Web scraping is an automated way of accessing web data. Instructions and strategy are specific to each website and oftentimes violate platform policy. I recommend the Chrome browser tool: CSS Selector. It helps identify exactly what you need to access from the website source code. In web scraping, oftentimes you can only access what you would normally see on the webpage (despite ability to hover or click through). API access is almost always preferred by both web scraper and data provider in that data is easier to access in a structured format and process.

1. Identify URL and patterns of URL path. For example, is the data you are looking for predictable in the URL path? If you need multiple pages of information, can you reliably guess what the URL will be when you need to access more pages?
2. R packages like rvest, XML, and httr can be helpful in simplifying web scraping requests
3. Establish a connection to the URL source page
4. Use the CSS selector to identify CSS or XPATH variables to help narrow your search. For example, if you are looking to scrape prices, identify what HTML containers have prices. These are often referred to as "nodes" and you can filter out a lot of the irrelevant data by specifying the node you are looking for
5. Convert filtered results to a text or table format for easier analysis

You may encounter issues with:
- HTTP vs HTTPS. I would default to HTTPS or you may encounter firewall issues. It is possible the data is only available through HTTP.
- Corporate Proxy. Elkay has its own corporate firewall which may find your activities suspicious. I have found success in identifying my credentials internally before attempting to connect to websites.
- Website terms & conditions. Web scraping often violates platform policies and so you may be blocked as a result or as a defensive measure, protecting against attacks. There are several strategies to avoid this.
-- Limit # of calls to the website
-- Limit amount of data requested from the website
-- Limit frequency of calls to the website
-- Use R's handshake (???) package which attempts to establish a fair common ground between web scrapers and website data providers


```{r rvest}
# Load packages
library(rvest)
library(httr)
library(RSelenium)

# Source web data
url <- "https://www.espn.com"
  scrape_hd1 <- read_html(url) %>%
    html_nodes("div") %>% #a #span #a div #div a
    html_text()
  
  #Manipulate user agent data(Mozilla, Chrome, IE etc), cookie acceptance, and encoding.
#Manipulate your source location (ip address, and server info)
# Let's set user agent to a super common one
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36"
#https://stackoverflow.com/questions/66975138/change-user-agent-when-using-rvestread-html
# Query webpage
bbc <- GET(paste(url,"&itemsperpage=96",sep=''),
           user_agent(ua))
```
